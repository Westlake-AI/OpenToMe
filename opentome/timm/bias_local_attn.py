"""
Biased Local Attention Implementation

é€šè¿‡æ‰©å±•ç»´åº¦çš„æ–¹å¼å®ç°å¸¦ bias çš„å±€éƒ¨çª—å£æ³¨æ„åŠ›ï¼Œåˆ©ç”¨ç»´åº¦å¯¹é½ä¼˜åŒ–æ€§èƒ½ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
    Q_ext = [q, âˆšD], K_ext = [k, bias]
    score = (Q_ext @ K_ext^T) / âˆšD = (qÂ·k + âˆšDÂ·bias) / âˆšD = qÂ·k/âˆšD + bias
    
ä¸ºäº†ä¿æŒ flash-attn çš„ Tensor Core ä¼˜åŒ–ï¼Œç‰©ç†ç»´åº¦å¯¹é½åˆ° 8 çš„å€æ•°ã€‚
"""

import math
from typing import Optional
import torch
import torch.nn as nn
from torch.jit import Final

# workspace ç¼“å­˜ï¼šé¿å…éå¯¹é½ head_dim æ¯æ¬¡æ„é€ /é”€æ¯ä¸´æ—¶å¼ é‡å¸¦æ¥çš„æ˜¾å­˜/æ—¶é—´å¼€é”€
_flash_pad_cache = {}


def _get_cached_buffer(cache: dict, key, shape, device, dtype, requires_grad: bool):
    """è·å–æˆ–åˆ›å»ºç¼“å­˜çš„ bufferï¼Œé¿å…é‡å¤åˆ†é…æ˜¾å­˜"""
    buf = cache.get(key)
    if buf is None or buf.shape != shape or buf.device != device or buf.dtype != dtype:
        buf = torch.empty(shape, device=device, dtype=dtype, requires_grad=requires_grad)
        cache[key] = buf
    else:
        # é‡æ–°æ¥å…¥ autogradï¼ˆè·¨è¿­ä»£ä½¿ç”¨éœ€è¦ detachï¼‰
        buf = buf.detach().requires_grad_(requires_grad)
    return buf


def clear_cache():
    """æ¸…ç†ç¼“å­˜ä»¥é‡Šæ”¾æ˜¾å­˜"""
    global _flash_pad_cache
    _flash_pad_cache.clear()


# ============================================================================
# Naive å®ç°ï¼ˆå›é€€è·¯å¾„ï¼Œä¸ä¾èµ– flash-attnï¼‰
# ============================================================================

def naive_biased_local_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    bias: torch.Tensor,
    local_window: int,
    *,
    dropout_p: float = 0.0,
    training: bool = False,
    x_dtype: Optional[torch.dtype] = None,
) -> torch.Tensor:
    """Naive å®ç°çš„ biased local attentionï¼ˆä½¿ç”¨ unfoldï¼Œä¸ä¾èµ– flash-attnï¼‰
    
    Args:
        q: Query tensor, shape (B, H, N, D) or (B, N, H, D)
        k: Key tensor, shape (B, H, N, D) or (B, N, H, D)
        v: Value tensor, shape (B, H, N, D) or (B, N, H, D)
        bias: Per-key bias, shape (B, N)
        local_window: å±€éƒ¨çª—å£å¤§å° h
        dropout_p: Dropout æ¦‚ç‡
        training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
        x_dtype: è¾“å‡ºæ•°æ®ç±»å‹
    
    Returns:
        Output tensor, shape ä¸è¾“å…¥æ ¼å¼ä¸€è‡´
    """
    import torch.nn.functional as F
    
    # è‡ªåŠ¨æ£€æµ‹è¾“å…¥æ ¼å¼
    assert q.ndim == 4, f"q must be 4D, got shape {q.shape}"
    
    if q.shape[1] > q.shape[2]:
        # (B, N, H, D) æ ¼å¼ -> è½¬æ¢ä¸º (B, H, N, D)
        B, N, H, D = q.shape
        input_format = "BNHD"
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
    else:
        # (B, H, N, D) æ ¼å¼
        B, H, N, D = q.shape
        input_format = "BHND"
    
    assert q.shape == k.shape == v.shape, f"q/k/v shape mismatch"
    assert bias.shape == (B, N), f"bias must be (B, N), got {bias.shape}"
    
    output_dtype = x_dtype if x_dtype is not None else q.dtype
    BH = B * H
    
    # Reshape: (B, H, N, D) -> (BH, N, D)
    q_flat = q.reshape(BH, N, D)
    k_flat = k.reshape(BH, N, D)
    v_flat = v.reshape(BH, N, D)
    
    # Padding
    h = local_window
    padded_k = F.pad(k_flat, (0, 0, h, h))
    padded_v = F.pad(v_flat, (0, 0, h, h))
    
    # Unfold to create windows: (BH, N, 2h+1, D)
    k_windows = padded_k.unfold(dimension=1, size=2 * h + 1, step=1)
    v_windows = padded_v.unfold(dimension=1, size=2 * h + 1, step=1)
    k_windows = k_windows.transpose(-1, -2)  # (BH, N, D, 2h+1)
    v_windows = v_windows.transpose(-1, -2)  # (BH, N, D, 2h+1)
    
    # Compute attention scores
    q_reshaped = q_flat.unsqueeze(2)  # (BH, N, 1, D)
    attn = (q_reshaped @ k_windows.transpose(-1, -2)).squeeze(2)  # (BH, N, 2h+1)
    
    # Add bias
    # bias: (B, N) -> (BH, N)
    bias_bh = bias.repeat_interleave(H, dim=0)
    # Pad bias and unfold
    padded_bias = F.pad(bias_bh, (h, h), mode='constant', value=-1e9)
    bias_windows = padded_bias.unfold(dimension=1, size=2 * h + 1, step=1)  # (BH, N, 2h+1)
    attn = attn + bias_windows
    
    # Boundary mask
    win_indices = torch.arange(-h, h + 1, device=q.device).view(1, -1)
    q_indices = torch.arange(N, device=q.device).view(-1, 1)
    abs_k_pos = q_indices + win_indices  # (N, 2h+1)
    mask = (abs_k_pos >= 0) & (abs_k_pos < N)
    attn = attn.masked_fill(~mask.unsqueeze(0), float('-inf'))
    
    # Softmax
    attn = attn.softmax(dim=-1)
    
    # Dropout
    if training and dropout_p > 0.0:
        attn = F.dropout(attn, p=dropout_p, training=True)
    
    # Apply attention to values
    attn_out_flat = (attn.unsqueeze(2) @ v_windows).squeeze(2)  # (BH, N, D)
    
    # Reshape back
    out = attn_out_flat.view(B, H, N, D)
    
    # Restore original format
    if input_format == "BNHD":
        out = out.transpose(1, 2)
    
    return out.to(output_dtype)


def naive_unbiased_local_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    local_window: int,
    *,
    dropout_p: float = 0.0,
    training: bool = False,
    x_dtype: Optional[torch.dtype] = None,
) -> torch.Tensor:
    """Naive å®ç°çš„ unbiased local attentionï¼ˆä½¿ç”¨ unfoldï¼Œä¸ä¾èµ– flash-attnï¼‰
    
    Args:
        q: Query tensor, shape (B, H, N, D) or (B, N, H, D)
        k: Key tensor, shape (B, H, N, D) or (B, N, H, D)
        v: Value tensor, shape (B, H, N, D) or (B, N, H, D)
        local_window: å±€éƒ¨çª—å£å¤§å° h
        dropout_p: Dropout æ¦‚ç‡
        training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
        x_dtype: è¾“å‡ºæ•°æ®ç±»å‹
    
    Returns:
        Output tensor, shape ä¸è¾“å…¥æ ¼å¼ä¸€è‡´
    """
    import torch.nn.functional as F
    
    # è‡ªåŠ¨æ£€æµ‹è¾“å…¥æ ¼å¼
    assert q.ndim == 4, f"q must be 4D, got shape {q.shape}"
    
    if q.shape[1] > q.shape[2]:
        # (B, N, H, D) æ ¼å¼ -> è½¬æ¢ä¸º (B, H, N, D)
        B, N, H, D = q.shape
        input_format = "BNHD"
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
    else:
        # (B, H, N, D) æ ¼å¼
        B, H, N, D = q.shape
        input_format = "BHND"
    
    assert q.shape == k.shape == v.shape, f"q/k/v shape mismatch"
    
    output_dtype = x_dtype if x_dtype is not None else q.dtype
    BH = B * H
    
    # Reshape: (B, H, N, D) -> (BH, N, D)
    q_flat = q.reshape(BH, N, D)
    k_flat = k.reshape(BH, N, D)
    v_flat = v.reshape(BH, N, D)
    
    # Padding
    h = local_window
    padded_k = F.pad(k_flat, (0, 0, h, h))
    padded_v = F.pad(v_flat, (0, 0, h, h))
    
    # Unfold to create windows: (BH, N, 2h+1, D)
    k_windows = padded_k.unfold(dimension=1, size=2 * h + 1, step=1)
    v_windows = padded_v.unfold(dimension=1, size=2 * h + 1, step=1)
    k_windows = k_windows.transpose(-1, -2)  # (BH, N, D, 2h+1)
    v_windows = v_windows.transpose(-1, -2)  # (BH, N, D, 2h+1)
    
    # Compute attention scores
    q_reshaped = q_flat.unsqueeze(2)  # (BH, N, 1, D)
    attn = (q_reshaped @ k_windows.transpose(-1, -2)).squeeze(2)  # (BH, N, 2h+1)
    
    # Boundary mask
    win_indices = torch.arange(-h, h + 1, device=q.device).view(1, -1)
    q_indices = torch.arange(N, device=q.device).view(-1, 1)
    abs_k_pos = q_indices + win_indices  # (N, 2h+1)
    mask = (abs_k_pos >= 0) & (abs_k_pos < N)
    attn = attn.masked_fill(~mask.unsqueeze(0), float('-inf'))
    
    # Softmax
    attn = attn.softmax(dim=-1)
    
    # Dropout
    if training and dropout_p > 0.0:
        attn = F.dropout(attn, p=dropout_p, training=True)
    
    # Apply attention to values
    attn_out_flat = (attn.unsqueeze(2) @ v_windows).squeeze(2)  # (BH, N, D)
    
    # Reshape back
    out = attn_out_flat.view(B, H, N, D)
    
    # Restore original format
    if input_format == "BNHD":
        out = out.transpose(1, 2)
    
    return out.to(output_dtype)


def biased_local_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    bias: torch.Tensor,
    local_window: int,
    *,
    logical_dim: Optional[int] = None,
    dropout_p: float = 0.0,
    training: bool = False,
    x_dtype: Optional[torch.dtype] = None,
) -> torch.Tensor:
    """å¸¦ bias çš„å±€éƒ¨çª—å£æ³¨æ„åŠ›ï¼ˆåŸºäº flash-attn ä¼˜åŒ–ï¼‰
    
    é€šè¿‡æ‰©å±• Q/K ç»´åº¦å®ç° per-key biasï¼ŒåŒæ—¶ä¿æŒ flash-attn çš„æ€§èƒ½ä¼˜åŒ–ã€‚
    
    æ•°å­¦åŸç†ï¼š
        Q_ext = [q, âˆšD], K_ext = [k, log(size)]
        score = (Q_ext @ K_ext^T) * (1/âˆšD)
              = (qÂ·k + âˆšDÂ·bias) / âˆšD
              = qÂ·k/âˆšD + bias  âœ“
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
        - é€»è¾‘ç»´åº¦ï¼šD + 1ï¼ˆéœ€è¦é¢å¤–ä¸€åˆ—å­˜ biasï¼‰
        - ç‰©ç†ç»´åº¦ï¼šå¯¹é½åˆ° 8 çš„å€æ•°ï¼ˆè§¦å‘ Tensor Core ä¼˜åŒ–ï¼‰
        - padding åˆ—å…¨é›¶ï¼šä¸å½±å“ attention è®¡ç®—
    
    Args:
        q: Query tensor, shape (B, N, H, D) or (B, H, N, D)
        k: Key tensor, shape (B, N, H, D) or (B, H, N, D)
        v: Value tensor, shape (B, N, H, D) or (B, H, N, D)
        bias: Per-key bias, shape (B, N)ï¼Œå°†ä½œä¸ºæ¯ä¸ª key ä½ç½®çš„åŠ æ€§åç½®
        local_window: å±€éƒ¨çª—å£å¤§å° hï¼Œæ¯ä¸ªä½ç½®åª attend åˆ° [-h, h] èŒƒå›´å†…çš„ tokens
        logical_dim: é€»è¾‘ç»´åº¦ï¼ˆè‹¥æä¾›ä¸”å°äºç‰©ç† Dï¼Œåˆ™è®¤ä¸ºè¾“å…¥å·²å¯¹é½/å¡«é›¶ï¼‰
        dropout_p: Dropout æ¦‚ç‡ï¼Œä»…åœ¨ training=True æ—¶ç”Ÿæ•ˆï¼ˆé»˜è®¤ 0.0ï¼‰
        training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œå½±å“ dropout è¡Œä¸º
        x_dtype: è¾“å‡ºæ•°æ®ç±»å‹ï¼ˆè‹¥ä¸º Noneï¼Œåˆ™ä½¿ç”¨è¾“å…¥ q çš„ç±»å‹ï¼‰
    
    Returns:
        Output tensor, shape (B, N, H, D) or (B, H, N, D) (ä¸è¾“å…¥æ ¼å¼ä¸€è‡´)
    
    Examples:
        >>> # å…¸å‹ç”¨æ³•ï¼ˆè¾“å‡ºç±»å‹ä¸è¾“å…¥ç›¸åŒï¼‰
        >>> q = torch.randn(2, 1024, 8, 64, device='cuda', dtype=torch.float16)
        >>> k = torch.randn(2, 1024, 8, 64, device='cuda', dtype=torch.float16)
        >>> v = torch.randn(2, 1024, 8, 64, device='cuda', dtype=torch.float16)
        >>> size_log = torch.randn(2, 1024, device='cuda', dtype=torch.float32)
        >>> out = biased_local_attention(q, k, v, size_log, local_window=16)
        >>> out.shape
        torch.Size([2, 1024, 8, 64])
        
        >>> # æ˜¾å¼æŒ‡å®šè¾“å‡ºç±»å‹ï¼ˆè¾“å…¥ fp32 è®¡ç®—ï¼Œè¾“å‡º fp16ï¼‰
        >>> q_fp32 = q.to(torch.float32)
        >>> k_fp32 = k.to(torch.float32)
        >>> v_fp32 = v.to(torch.float32)
        >>> out = biased_local_attention(
        ...     q_fp32, k_fp32, v_fp32, size_log, 
        ...     local_window=16, 
        ...     x_dtype=torch.float16
        ... )
        >>> out.dtype
        torch.float16
    """
    try:
        from flash_attn import flash_attn_func
    except Exception:
        try:
            from flash_attn.flash_attn_interface import flash_attn_func
        except Exception:
            # flash-attn ä¸å¯ç”¨ï¼Œå›é€€åˆ° naive å®ç°
            return naive_biased_local_attention(
                q, k, v, bias, local_window,
                dropout_p=dropout_p,
                training=training,
                x_dtype=x_dtype,
            )
    
    # è‡ªåŠ¨æ£€æµ‹è¾“å…¥æ ¼å¼ï¼š(B, N, H, D) vs (B, H, N, D)
    # flash-attn æœŸæœ› (B, H, N, D) æˆ–é€šè¿‡ transpose è½¬æ¢
    assert q.ndim == 4, f"q must be 4D, got shape {q.shape}"
    
    # åˆ¤æ–­æ ¼å¼ï¼šå¦‚æœç¬¬äºŒä¸ªç»´åº¦è¿œå¤§äºç¬¬ä¸‰ä¸ªç»´åº¦ï¼Œåˆ™è®¤ä¸ºæ˜¯ (B, N, H, D)
    if q.shape[1] > q.shape[2]:
        # (B, N, H, D) æ ¼å¼
        B, N, H, D = q.shape
        input_format = "BNHD"
    else:
        # (B, H, N, D) æ ¼å¼
        # ğŸ”§ FIX: å˜é‡ååº”è¯¥åæ˜ è¯­ä¹‰å«ä¹‰ï¼Œä¸æ˜¯ tensor çš„ç»´åº¦é¡ºåº
        # B=batch, N=seq_len, H=heads, D=head_dim
        temp_B, temp_H, temp_N, temp_D = q.shape
        B, N, H, D = temp_B, temp_N, temp_H, temp_D  # é‡æ–°æ’åˆ—ä»¥åŒ¹é…è¯­ä¹‰
        input_format = "BHND"
    
    # éªŒè¯å½¢çŠ¶ä¸€è‡´æ€§
    assert q.shape == k.shape == v.shape, f"q/k/v shape mismatch: {q.shape}, {k.shape}, {v.shape}"
    assert bias.shape == (B, N), f"bias must be (B, N), got {bias.shape}"
    
    # ç¡®å®šé€»è¾‘ç»´åº¦å’Œ softmax scale
    D_logic = logical_dim if logical_dim is not None else D
    softmax_scale = 1.0 / math.sqrt(D_logic)
    
    # ç¡®å®šè¾“å‡ºæ•°æ®ç±»å‹
    output_dtype = x_dtype if x_dtype is not None else q.dtype
    
    # æ•°æ®ç±»å‹å¤„ç†ï¼šflash-attn åªæ”¯æŒ fp16/bf16
    # å¦‚æœè¾“å…¥ä¸æ˜¯ fp16/bf16ï¼Œè½¬æ¢ä¸º fp16
    if q.dtype not in (torch.float16, torch.bfloat16):
        target_dtype = torch.float16  # é»˜è®¤ä½¿ç”¨ fp16
        q = q.to(target_dtype)
        k = k.to(target_dtype)
        v = v.to(target_dtype)
    
    # bias è½¬æ¢ä¸ºä¸ k ç›¸åŒçš„ç±»å‹
    if bias.dtype != k.dtype:
        bias = bias.to(k.dtype)
    
    # ç¡®ä¿è¾“å…¥æ ¼å¼ä¸º (B, H, N, D) ä»¥åŒ¹é… flash-attn
    if input_format == "BNHD":
        # (B, N, H, D) -> (B, H, N, D)
        q = q.transpose(1, 2).contiguous()
        k = k.transpose(1, 2).contiguous()
        v = v.transpose(1, 2).contiguous()
    
    # ========================================================================
    # å¿«é€Ÿè·¯å¾„ï¼šè‹¥è°ƒç”¨è€…å·²æä¾›å¯¹é½åçš„ç‰©ç†ç»´åº¦ï¼ˆD%8==0 ä¸” D_logic < Dï¼‰
    # åˆ™ç›´æ¥åŸåœ°å†™å…¥ bias åˆ—å¹¶è°ƒç”¨ flashï¼Œé¿å…å¤åˆ¶
    # ========================================================================
    if D % 8 == 0 and D_logic < D:
        q = q.clone() if not q.is_contiguous() else q
        k = k.clone() if not k.is_contiguous() else k
        v = v.clone() if not v.is_contiguous() else v
        
        # åœ¨ç¬¬ D_logic åˆ—å†™å…¥ bias
        # æ­¤æ—¶ q,k å½¢çŠ¶: (B, H, N, D)
        q[..., D_logic] = math.sqrt(D_logic)
        # ğŸ”§ FIX: bias (B, N) â†’ (B, 1, N) æ‰èƒ½ broadcast åˆ° (B, H, N)
        k[..., D_logic] = bias.unsqueeze(1).to(k.dtype)
        
        # å…¶ä½™åˆ—ç½®é›¶
        if D > D_logic + 1:
            q[..., D_logic + 1:] = 0
            k[..., D_logic + 1:] = 0
        
        # V ä¹Ÿéœ€è¦ padding åˆ—ç½®é›¶
        v_phys = v
        if D > D_logic:
            v_phys = v.clone() if not v.is_contiguous() else v
            v_phys[..., D_logic:] = 0
        
        out_ext = flash_attn_func(
            q, k, v_phys,
            dropout_p=dropout_p if training else 0.0,
            softmax_scale=softmax_scale,
            causal=False,
            window_size=(local_window, local_window),
            deterministic=not training,
        )
        out = out_ext[..., :D_logic]
        
        # æ¢å¤åŸå§‹æ ¼å¼
        if input_format == "BNHD":
            out = out.transpose(1, 2).contiguous()
        
        return out.to(output_dtype)
    # import pdb;pdb.set_trace()
    # ========================================================================
    # é€šç”¨è·¯å¾„ï¼šæ„é€ å¯¹é½åˆ° 8 çš„å€æ•°çš„ç‰©ç†å¼ é‡
    # ========================================================================
    D_ext = D_logic + 1  # é€»è¾‘ä¸Šéœ€è¦æ‰©å±•ä¸€åˆ—
    D_phys = ((D_ext + 7) // 8) * 8  # å‘ä¸Šå¯¹é½åˆ° 8 çš„å€æ•°
    
    # ğŸ”§ FIX: æ­¤æ—¶ q,k,v å·²ç»ç»Ÿä¸€ä¸º (B, H, N, D) æ ¼å¼ï¼ˆç»è¿‡ line 357-361 å¤„ç†ï¼‰
    # åˆ›å»ºå¯¹é½åçš„ bufferï¼Œä½¿ç”¨ (B, H, N, D_phys) é¡ºåº
    # 
    # æ³¨æ„ï¼šè®­ç»ƒæ—¶ä¸ä½¿ç”¨ç¼“å­˜ï¼Œå› ä¸º Flash Attention ä¼šä¿å­˜è¾“å…¥ç”¨äº backwardï¼Œ
    # ç¼“å­˜ä¼šå¯¼è‡´å¤šä¸ª iteration å…±äº«åŒä¸€å—å†…å­˜ï¼Œäº§ç”Ÿ "modified by inplace" é”™è¯¯ã€‚
    # æ¨ç†æ—¶å¯ä»¥ä½¿ç”¨ç¼“å­˜ï¼Œå› ä¸ºä¸éœ€è¦æ¢¯åº¦ã€‚
    if training:
        # è®­ç»ƒæ¨¡å¼ï¼šæ¯æ¬¡åˆ›å»ºæ–° tensor
        q_phys = torch.zeros((B, H, N, D_phys), device=q.device, dtype=q.dtype)
        k_phys = torch.zeros((B, H, N, D_phys), device=k.device, dtype=k.dtype)
        v_phys = torch.zeros((B, H, N, D_phys), device=v.device, dtype=v.dtype)
    else:
        # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ç¼“å­˜ä¼˜åŒ–
        key = (q.device, q.dtype, B, H, N, D_phys)
        q_phys = _get_cached_buffer(
                _flash_pad_cache, ('q',) + key, (B, H, N, D_phys),
                q.device, q.dtype, requires_grad=False
        )
        k_phys = _get_cached_buffer(
                _flash_pad_cache, ('k',) + key, (B, H, N, D_phys),
                k.device, k.dtype, requires_grad=False
        )
        v_phys = _get_cached_buffer(
                _flash_pad_cache, ('v',) + key, (B, H, N, D_phys),
                v.device, v.dtype, requires_grad=False
        )
    
    # æ‹·è´åŸå§‹æ•°æ®
    q_phys[..., :D] = q
    k_phys[..., :D] = k
    
    # åœ¨ç¬¬ D_logic åˆ—å¡«å…… biasï¼ˆè¿™æ˜¯å…³é”®ï¼ï¼‰
    # q_phys, k_phys å½¢çŠ¶: (B, H, N, D_phys)
    q_phys[..., D_logic] = math.sqrt(D_logic)
    # ğŸ”§ FIX: bias (B, N) â†’ (B, 1, N) æ‰èƒ½ broadcast åˆ° (B, H, N)
    k_phys[..., D_logic] = bias.unsqueeze(1).to(k.dtype)
    
    # å…¶ä½™ padding åˆ—ç½®é›¶ï¼ˆæ¬ºéª— flash-attnï¼Œè®©å®ƒèµ°å¿«é€Ÿè·¯å¾„ï¼‰
    if D_phys > D_ext:
        q_phys[..., D_ext:D_phys] = 0
        k_phys[..., D_ext:D_phys] = 0
    
    # V çš„æ‰€æœ‰ padding åˆ—éƒ½è¦ç½®é›¶
    v_phys.zero_()
    v_phys[..., :D] = v
    
    # è°ƒç”¨ flash-attnï¼ˆçœ‹åˆ°çš„æ˜¯å¯¹é½åçš„ D_phys ç»´åº¦ï¼‰
    out_ext = flash_attn_func(
        q_phys, k_phys, v_phys,
        dropout_p=dropout_p if training else 0.0,
        softmax_scale=softmax_scale,
        causal=False,
        window_size=(local_window, local_window),
        deterministic=not training,
    )  # (B, H, N, D_phys)
    
    # åªè¿”å›é€»è¾‘ç»´åº¦çš„å‰ D åˆ—ï¼ˆä¸¢å¼ƒ paddingï¼‰
    out = out_ext[..., :D]
    
    # æ¢å¤åŸå§‹æ ¼å¼
    if input_format == "BNHD":
        out = out.transpose(1, 2).contiguous()  # (B, H, N, D) -> (B, N, H, D)
    
    return out.to(output_dtype)


def unbiased_local_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    local_window: int,
    *,
    logical_dim: Optional[int] = None,
    dropout_p: float = 0.0,
    training: bool = False,
    x_dtype: Optional[torch.dtype] = None,
) -> torch.Tensor:
    """ä¸å¸¦ bias çš„å±€éƒ¨çª—å£æ³¨æ„åŠ›ï¼ˆåŸºäº flash-attnï¼‰
    
    Args:
        q: Query tensor, shape (B, N, H, D) or (B, H, N, D)
        k: Key tensor, shape (B, N, H, D) or (B, H, N, D)
        v: Value tensor, shape (B, N, H, D) or (B, H, N, D)
        local_window: å±€éƒ¨çª—å£å¤§å° h
        logical_dim: é€»è¾‘ç»´åº¦ï¼ˆè‹¥æä¾›ä¸”å°äºç‰©ç† Dï¼Œåˆ™æŒ‰é€»è¾‘ç»´åº¦è®¡ç®— scaleï¼‰
        dropout_p: Dropout æ¦‚ç‡ï¼Œä»…åœ¨ training=True æ—¶ç”Ÿæ•ˆï¼ˆé»˜è®¤ 0.0ï¼‰
        training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œå½±å“ dropout è¡Œä¸º
        x_dtype: è¾“å‡ºæ•°æ®ç±»å‹ï¼ˆè‹¥ä¸º Noneï¼Œåˆ™ä½¿ç”¨è¾“å…¥ q çš„ç±»å‹ï¼‰
    
    Returns:
        Output tensor, shape (B, N, H, D) or (B, H, N, D) (ä¸è¾“å…¥æ ¼å¼ä¸€è‡´)
    """
    try:
        from flash_attn import flash_attn_func
    except Exception:
        try:
            from flash_attn.flash_attn_interface import flash_attn_func
        except Exception:
            # flash-attn ä¸å¯ç”¨ï¼Œå›é€€åˆ° naive å®ç°
            return naive_unbiased_local_attention(
                q, k, v, local_window,
                dropout_p=dropout_p,
                training=training,
                x_dtype=x_dtype,
            )
    
    # è‡ªåŠ¨æ£€æµ‹è¾“å…¥æ ¼å¼
    assert q.ndim == 4, f"q must be 4D, got shape {q.shape}"
    
    if q.shape[1] > q.shape[2]:
        # (B, N, H, D) æ ¼å¼
        B, N, H, D = q.shape
        input_format = "BNHD"
    else:
        # (B, H, N, D) æ ¼å¼
        B, H, N, D = q.shape
        input_format = "BHND"
    
    assert q.shape == k.shape == v.shape, f"q/k/v shape mismatch"
    
    D_logic = logical_dim if logical_dim is not None else D
    softmax_scale = 1.0 / math.sqrt(D_logic)
    
    # ç¡®å®šè¾“å‡ºæ•°æ®ç±»å‹
    output_dtype = x_dtype if x_dtype is not None else q.dtype
    
    # æ•°æ®ç±»å‹å¤„ç†ï¼šflash-attn åªæ”¯æŒ fp16/bf16
    if q.dtype not in (torch.float16, torch.bfloat16):
        target_dtype = torch.float16
        q = q.to(target_dtype)
        k = k.to(target_dtype)
        v = v.to(target_dtype)
    
    # ç¡®ä¿è¾“å…¥æ ¼å¼ä¸º (B, H, N, D)
    if input_format == "BNHD":
        q = q.transpose(1, 2).contiguous()
        k = k.transpose(1, 2).contiguous()
        v = v.transpose(1, 2).contiguous()
    
    # è‹¥ head_dim å·²å¯¹é½ï¼Œç›´æ¥è°ƒç”¨
    if D % 8 == 0:
        out = flash_attn_func(
            q, k, v,
            dropout_p=dropout_p if training else 0.0,
            softmax_scale=softmax_scale,
            causal=False,
            window_size=(local_window, local_window),
            deterministic=not training,
        )
        
        # æ¢å¤åŸå§‹æ ¼å¼
        if input_format == "BNHD":
            out = out.transpose(1, 2).contiguous()
        
        return out.to(output_dtype)
    
    # éå¯¹é½åœºæ™¯ï¼šç‰©ç†å¯¹é½åˆ° 8 çš„å€æ•°
    D_phys = ((D + 7) // 8) * 8
    
    # ğŸ”§ FIX: æ­¤æ—¶ q,k,v å·²ç»æ˜¯ (B, H, N, D) æ ¼å¼ï¼ˆç»è¿‡ line 548-551 å¤„ç†ï¼‰
    # æ³¨æ„ï¼šè®­ç»ƒæ—¶ä¸ä½¿ç”¨ç¼“å­˜ï¼Œå› ä¸º Flash Attention ä¼šä¿å­˜è¾“å…¥ç”¨äº backwardï¼Œ
    # ç¼“å­˜ä¼šå¯¼è‡´å¤šä¸ª iteration å…±äº«åŒä¸€å—å†…å­˜ï¼Œäº§ç”Ÿ "modified by inplace" é”™è¯¯ã€‚
    if training:
        # è®­ç»ƒæ¨¡å¼ï¼šæ¯æ¬¡åˆ›å»ºæ–° tensor
        q_phys = torch.zeros((B, H, N, D_phys), device=q.device, dtype=q.dtype)
        k_phys = torch.zeros((B, H, N, D_phys), device=k.device, dtype=k.dtype)
        v_phys = torch.zeros((B, H, N, D_phys), device=v.device, dtype=v.dtype)
    else:
        # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ç¼“å­˜ä¼˜åŒ–
        key = (q.device, q.dtype, B, H, N, D_phys)
        q_phys = _get_cached_buffer(
            _flash_pad_cache, ('q_ub',) + key, (B, H, N, D_phys),
            q.device, q.dtype, requires_grad=False
        )
        k_phys = _get_cached_buffer(
            _flash_pad_cache, ('k_ub',) + key, (B, H, N, D_phys),
            k.device, k.dtype, requires_grad=False
        )
        v_phys = _get_cached_buffer(
            _flash_pad_cache, ('v_ub',) + key, (B, H, N, D_phys),
            v.device, v.dtype, requires_grad=False
        )
    
    q_phys[..., :D] = q
    k_phys[..., :D] = k
    v_phys[..., :D] = v
    if D_phys > D:
        q_phys[..., D:D_phys] = 0
        k_phys[..., D:D_phys] = 0
        v_phys[..., D:D_phys] = 0
    
    out_ext = flash_attn_func(
        q_phys, k_phys, v_phys,
        dropout_p=dropout_p if training else 0.0,
        softmax_scale=softmax_scale,
        causal=False,
        window_size=(local_window, local_window),
        deterministic=not training,
    )
    
    out = out_ext[..., :D]
    
    # æ¢å¤åŸå§‹æ ¼å¼
    if input_format == "BNHD":
        out = out.transpose(1, 2).contiguous()
    
    return out.to(output_dtype)


# ============================================================================
# LocalAttention & LocalBlock: ä½¿ç”¨ unbiased local attention çš„ Transformer Block
# ============================================================================

class LocalAttention(nn.Module):
    """Local window attention using unbiased local attention
    
    ç±»ä¼¼äº timm.Attentionï¼Œä½†ä½¿ç”¨å±€éƒ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ã€‚
    """
    fused_attn: Final[bool]
    
    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = False,
        qk_norm: bool = False,
        attn_drop: float = 0.0,
        proj_drop: float = 0.0,
        norm_layer: nn.Module = nn.LayerNorm,
        local_window: int = 16,
    ) -> None:
        """
        Args:
            dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            qkv_bias: æ˜¯å¦åœ¨ QKV æŠ•å½±ä¸­ä½¿ç”¨ bias
            qk_norm: æ˜¯å¦å¯¹ Q/K è¿›è¡Œå½’ä¸€åŒ–
            attn_drop: Attention dropout æ¦‚ç‡
            proj_drop: è¾“å‡ºæŠ•å½± dropout æ¦‚ç‡
            norm_layer: å½’ä¸€åŒ–å±‚ç±»å‹
            local_window: å±€éƒ¨çª—å£å¤§å°
        """
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.fused_attn = False  # ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ local attention
        self.local_window = local_window
        
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, N, C)
        
        Returns:
            (B, N, C)
        """
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0)  # (B, H, N, D)
        q, k = self.q_norm(q), self.k_norm(k)
        
        # ä½¿ç”¨ unbiased local attention
        q = q * self.scale
        x = unbiased_local_attention(
            q, k, v,
            local_window=self.local_window,
            dropout_p=self.attn_drop.p,
            training=self.training,
        )  # (B, H, N, D)
        
        # (B, H, N, D) -> (B, N, H, D) -> (B, N, C)
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class LocalBlock(nn.Module):
    """Transformer Block with Local Window Attention
    
    ä¸ timm.Block ç±»ä¼¼ï¼Œä½†ä½¿ç”¨å±€éƒ¨çª—å£æ³¨æ„åŠ›ã€‚
    """
    
    def __init__(
        self,
        dim: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = False,
        qk_norm: bool = False,
        proj_drop: float = 0.0,
        attn_drop: float = 0.0,
        init_values: Optional[float] = None,
        drop_path: float = 0.0,
        act_layer: nn.Module = nn.GELU,
        norm_layer: nn.Module = nn.LayerNorm,
        local_window: int = 16,
    ) -> None:
        """
        Args:
            dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            mlp_ratio: MLP éšè—å±‚ç»´åº¦ç›¸å¯¹äºè¾“å…¥ç»´åº¦çš„æ¯”ä¾‹
            qkv_bias: æ˜¯å¦åœ¨ QKV æŠ•å½±ä¸­ä½¿ç”¨ bias
            qk_norm: æ˜¯å¦å¯¹ Q/K è¿›è¡Œå½’ä¸€åŒ–
            proj_drop: æŠ•å½±å±‚ dropout æ¦‚ç‡
            attn_drop: Attention dropout æ¦‚ç‡
            init_values: LayerScale åˆå§‹åŒ–å€¼ï¼ˆNone è¡¨ç¤ºä¸ä½¿ç”¨ LayerScaleï¼‰
            drop_path: DropPath æ¦‚ç‡
            act_layer: MLP æ¿€æ´»å‡½æ•°
            norm_layer: å½’ä¸€åŒ–å±‚ç±»å‹
            local_window: å±€éƒ¨çª—å£å¤§å°
        """
        super().__init__()
        
        try:
            from timm.layers import Mlp, DropPath
            from timm.models.vision_transformer import LayerScale
        except ImportError:
            raise ImportError("Please install timm: pip install timm")
        
        self.norm1 = norm_layer(dim)
        self.attn = LocalAttention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_norm=qk_norm,
            attn_drop=attn_drop,
            proj_drop=proj_drop,
            norm_layer=norm_layer,
            local_window=local_window,
        )
        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        
        self.norm2 = norm_layer(dim)
        self.mlp = Mlp(
            in_features=dim,
            hidden_features=int(dim * mlp_ratio),
            act_layer=act_layer,
            drop=proj_drop,
        )
        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, N, C)
        
        Returns:
            (B, N, C)
        """
        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
        return x