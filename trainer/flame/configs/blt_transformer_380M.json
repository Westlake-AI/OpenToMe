{
  "model_type": "blt",
  "vocab_size": 260,
  "max_position_embeddings": 4096,
  "initializer_range": 0.02,
  "tie_word_embeddings": false,
  "patch_in_forward": true,
  "patch_size": 4,
  "patching_mode": "entropy",
  "patching_threshold": 1.335442066192627,
  "patching_batch_size": 1,
  "max_patch_length": null,
  "patching_device": "cuda",
  "realtime_patching": true,
  "patching_threshold_add": null,
  "monotonicity": false,
  "cross_attn_k": 2,
  "encoder_hash_byte_group_size": [3, 4, 5, 6, 7, 8],
  "encoder_hash_byte_group_vocab": 500,
  "encoder_hash_byte_group_nb_functions": 1,
  "patcher_config": {
    "model_type": "blt_patcher",
    "vocab_size": 260,
    "hidden_size": 512,
    "num_hidden_layers": 7,
    "num_attention_heads": 8,
    "num_key_value_heads": 8,
    "max_position_embeddings": 8192,
    "rms_norm_eps": 1e-5,
    "dropout": 0.0,
    "intermediate_size": 1365,
    "hidden_act": "silu",
    "initializer_range": 0.02,
    "rope_parameters": {"rope_type": "default",
                        "rope_theta": 500000
                        }
  },
  "encoder_config": {
    "model_type": "blt_local_encoder",
    "vocab_size": 260,
    "hidden_size": 512,
    "hidden_size_global": 1024,
    "num_hidden_layers": 1,
    "num_attention_heads": 8,
    "num_key_value_heads": 8,
    "head_dim": 64,
    "intermediate_size": 1365,
    "rms_norm_eps": 1e-5,
    "dropout": 0.0,
    "max_position_embeddings": 24576,
    "cross_attn_all_layers": false,
    "cross_attn_k": 2,
    "hidden_act": "silu",
    "initializer_range": 0.02,
    "rope_parameters": {"rope_type": "default",
                        "rope_theta": 500000
                        }
  },
  "decoder_config": {
    "model_type": "blt_local_decoder",
    "vocab_size": 260,
    "hidden_size": 512,
    "hidden_size_global": 1024,
    "num_hidden_layers": 9,
    "num_attention_heads": 8,
    "num_key_value_heads": 8,
    "head_dim": 64,
    "intermediate_size": 1365,
    "rms_norm_eps": 1e-5,
    "dropout": 0.0,
    "max_position_embeddings": 24576,
    "cross_attn_all_layers": true,
    "cross_attn_k": 2,
    "hidden_act": "silu",
    "initializer_range": 0.02,
    "rope_parameters": {"rope_type": "default",
                        "rope_theta": 500000
                        }
  },
  "global_config": {
    "model_type": "blt_global_transformer",
    "hidden_size": 1024,
    "num_hidden_layers": 25,
    "num_attention_heads": 8,
    "num_key_value_heads": 8,
    "head_dim": 128,
    "intermediate_size": 2731,
    "rms_norm_eps": 1e-5,
    "dropout": 0.0,
    "max_position_embeddings": 4096,
    "hidden_act": "silu",
    "initializer_range": 0.02,
    "rope_parameters": {"rope_type": "default",
                        "rope_theta": 500000
                        },
    "encoder_cross_output_size": null
  }
}
