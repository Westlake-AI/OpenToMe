# OpenToMe

### Install

```bash
git clone https://github.com/Westlake-AI/OpenToMe.git
conda create -n opentome python=3.10.0
conda activate opentome
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124
```

## Developing & Evaluation

1. **Install from `docs/requirements.txt`**

    è¿™ä¸ªæ˜¯æˆ‘å½“å‰ç¯å¢ƒå†…æ‰€æœ‰çš„å®‰è£…åŒ…ï¼Œå¯ä»¥ç”¨æ¥å¯¹é½ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥`pip install -r requirements.txt`æ¥å®‰è£…ç¯å¢ƒï¼Œä½†æ˜¯æˆ‘ä¸ªäººä¸æ¨èï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨å®‰è£…åŒ…ä¹‹é—´é¡ºåºçš„é—®é¢˜ã€‚

2. **Install from scoure**
    - **FLA** å…ˆæŒ‰ç…§ä¸Šé¢bashåˆ›å»ºbase evnsï¼Œç„¶åè®°å¾—git cloneä¸€ä¸‹flash-linear-attentionï¼Œç„¶åæŒ‰ç…§FLAçš„éœ€æ±‚å®‰è£…ç¯å¢ƒ [**flash-linear-attention README.md**](https://github.com/fla-org/flash-linear-attention/blob/main/README.md).

    - **flame** ä¹Ÿæ˜¯æŒ‰ç…§å®ƒé‡Œé¢çš„README.mdæ¥æ›´æ–°: `pip install -e .`ï¼ˆè¿™ä¸ªæˆ‘æœ‰ç‚¹è®°ä¸æ¸…éœ€è¦éœ€è¦äº†ï¼Œä½ å¯ä»¥å…ˆä¸ç”¨å†å®‰è£…è¿™ä¸ªè¯•è¯•æŠ¥ä¸æŠ¥é”™ï¼‰

    - **lmms-evaluation-harness** è¿™ä¸ªä¹Ÿæ˜¯ä¸€æ ·å…ˆ`git clone https://github.com/EleutherAI/lm-evaluation-harness.git`ï¼Œç„¶åæŒ‰ç…§README.mdå®‰è£…å³å¯: `pip install -e .`ï¼Œå¦‚æœéœ€è¦eval LongBenchçš„è¯ï¼Œéœ€è¦å®‰è£…ç‰¹å®šçš„åŒ…: `pip install lm_eval['longbench']`

    - [Important] Install specific version of torchtitan: `pip install git+https://github.com/pytorch/torchtitan.git@0b44d4c`

    - **Flash Attention** è¿™ä¸ªæ˜¯éœ€è¦å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ï¼Œç›´æ¥å¤åˆ¶ç²˜è´´å³å¯: `pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl`

---

### Training
Here is an example of training with flash linear attention by [**flame**](https://github.com/fla-org/flame)
```bash
#!/usr/bin/bash
export HF_ENDPOINT=https://hf-mirror.com

NNODE=1 NGPU=8 LOG_RANK=0 bash train.sh \
  --job.config_file flame/models/fla.toml \
  --job.dump_folder RESULTS/PATH \
  --model.config configs/gla_340M.json \
  --model.tokenizer_path TOKENIZER/PATH \
  --optimizer.name AdamW \
  --optimizer.eps 1e-15 \
  --optimizer.lr 3e-4 \
  --lr_scheduler.warmup_steps 1024 \
  --lr_scheduler.lr_min 0.1 \
  --lr_scheduler.decay_type cosine \
  --training.batch_size 32 \
  --training.seq_len 2048 \
  --training.gradient_accumulation_steps 1 \
  --training.steps 20480 \
  --training.max_norm 1.0 \
  --training.skip_nan_inf \
  --training.dataset DATASET/PATH \
  --training.dataset_name default \
  --training.dataset_split train \
  --training.streaming \
  --training.num_workers 32 \
  --training.prefetch_factor 2 \
  --training.seed 42 \
  --training.compile \
  --training.tensor_parallel_degree 1 \
  --training.disable_loss_parallel \
  --checkpoint.interval 2048 \
  --checkpoint.load_step -1 \
  --metrics.log_freq 1
```

### Evaluation of PPL and Common-sense Resaoning / QA
The evaluation we follow up with the [**flash-linear-attention**](https://github.com/fla-org/flash-linear-attention/blob/main/README.md). Please confirm that the requirements for [**lmms-eval-harness**](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/README.md?plain=1) are satisfied.

Here is an example of PPL evaluation by GLA model.
```bash
#!/usr/bin/bash
export HF_ENDPOINT=https://hf-mirror.com
MODEL='MODEL/PATH'

python -m harness --model hf \
    --model_args pretrained=$MODEL,dtype=bfloat16 \
    --tasks wikitext \
    --batch_size 64 \
    --num_fewshot 0 \
    --device cuda \
    --show_config
```

### Important [ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ] å››é¢—æ˜Ÿ

å› ä¸º`transformers`ä¸æ”¯æŒflash-attention-linearçš„Configsï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™è¾¹æ˜¯ç›´æ¥å¯¼å…¥osç„¶åæ‰‹åŠ¨æ›¿æ¢çš„ã€‚æ‰€ä»¥ä½ è¿™è¾¹åœ¨è·‘çš„æ—¶å€™è¦æ³¨æ„bashè„šæœ¬å†…éœ€è¦å¯¼å…¥ï¼š
```
export BACKBONE=MODEL_NAME
echo $BACKBON
```

For example:
```bash
#!/usr/bin/bash
export HF_ENDPOINT=https://hf-mirror.com
export BACKBONE=delta_net_340M
echo $BACKBONE
NNODE=1 NGPU=8 LOG_RANK=0 bash train.sh \
  --job.config_file flame/models/fla.toml \
  --job.dump_folder exp/delta_net_340M_10B/batch1.seqlen32768.grad_acc2.warmup1024.update1.steps20480.lr4e-4 \ # ä¿å­˜è·¯å¾„ \
  --model.config configs/delta_net_340M.json \  # configæ–‡ä»¶ \
  --model.tokenizer_path /yuchang/lsy_jx/.cache/models/delta_net-1.3B-100B \
  --optimizer.name AdamW \
  --optimizer.eps 1e-15 \
  --optimizer.lr 3e-4 \
  --lr_scheduler.warmup_steps 1024 \
  --lr_scheduler.lr_min 0.1 \
  --lr_scheduler.decay_type cosine \
  --training.batch_size 1 \
  --training.seq_len 32768 \
  --training.context_len 4096 \
  --training.varlen \
  --training.gradient_accumulation_steps 2 \
  --training.steps 30720 \
  --training.max_norm 1.0 \
  --training.skip_nan_inf \
  --training.dataset /ssdwork/yuchang/fineweb-edu/sample/100BT \  # æ•°æ®é›†è·¯å¾„ \
  --training.dataset_name default \
  --training.dataset_split train \
  --training.num_workers 32 \
  --training.prefetch_factor 2 \
  --training.seed 42 \
  --training.compile \
  --checkpoint.interval 15360 \
  --checkpoint.load_step -1 \
  --checkpoint.keep_latest_k 2 \
  --metrics.log_freq 1
```
Evaluationçš„æ—¶å€™ä¹Ÿæ˜¯ä¸€æ ·ï¼Œè¿™è¾¹æˆ‘åº”è¯¥éƒ½å¸®ä½ å†™å¥½äº†çš„ï¼Œé—®é¢˜ä¸å¤§ã€‚

### Support Models/Tokenizer 
- âœ… Transformer++
- âœ… GLA
- âœ… DeltaNet
- âœ… Gated-DeltaNet
- âœ… BLT (byte-level)
- âŒ Qwen3-NeXt
- âœ… LLaMA-based Tokenizer
- âœ… Byte-level Tokenizer

è¯´åˆ°Tokenizerçš„ä¸åŒï¼Œåªéœ€è¦`export TOKENIZER_NAME=blt`å³å¯

### Setups [ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ] äº”é¢—æ˜Ÿ

- **Model Size: 350M** & **1.5B**
    - **340M** 
        1. Total training tokens: ~15B 
        2. batch size: ~0.5M
        3. warmup: ~0.5B
    - **1.5B**
        1. Total training tokens: ~100B
        2. batch size: ~2M 
        2. warmup: ~1B

- **AdamW, LR, wd, grad clip, cosine scheduler, LR_max** æŒ‰ç…§æˆ‘350Mçš„æ¥å°±è¡Œï¼Œä¹Ÿå°±æ˜¯è¯´ä½ è·‘1.5Bçš„æ—¶å€™éœ€è¦ä¿®æ”¹çš„åªæœ‰configs, save path, warmup_steps, seq_len, gradient_accumulation_steps, stepså³å¯ï¼Œæœ‰ä¸ªè®¡ç®—å…¬å¼ä¾›ä½ å‚è€ƒï¼š
```bash
# ==========================================
# è®­ç»ƒè¶…å‚æ•°è®¡ç®—è¯´æ˜ (100B ç›®æ ‡)
# ==========================================
# 1. å•æ­¥ Token æ•° (Total Batch Size):
#    1 (BS) * 32768 (SeqLen) * 4 (GPU) * 16 (GA) = 2,097,152 (2M Tokens)
# 2. Warmup æ­¥æ•° (1B ç›®æ ‡):
#    1,000,000,000 / 2,097,152 â‰ˆ 477 Steps
# 3. æ€»æ­¥æ•° (100B ç›®æ ‡):
#    100,000,000,000 / 2,097,152 â‰ˆ 47,684 Steps
# ==========================================
```

*ç›®å‰OpenToMe repoä¸­å¤§éƒ¨åˆ†åªæœ‰350Mçš„bashè„šæœ¬ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªgated-deltenet-1b.shå¯ä»¥ä½œä¸ºä½ çš„å‚è€ƒï¼ˆglaï¼‰é‚£ä¸ªæœ‰é—®é¢˜éœ€æ³¨æ„ã€‚å¦‚æœæƒ³ç¡®è®¤å‚æ•°åˆ°åº•æ˜¯å¦æ­£ç¡®ï¼Œflameè´´å¿ƒçš„æä¾›äº†ä½ è®­ç»ƒå‚æ•°å¯¹åº”tokensçš„æ•°é‡ï¼Œåœ¨ä½ è¿è¡Œçš„æ—¶å€™ï¼Œä»–ä¼šæ‰“å°å‡ºæ¥ï¼Œè§æˆ‘é£ä¹¦çš„é‚£ä¸ªå›¾ã€‚è·‘ä¹‹å‰ä¸€å®šè¦æ³¨æ„setupæ˜¯å¦æ­£ç¡®ï¼å¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜å¯ä»¥åœ¨é£ä¹¦é‡Œé¢@æˆ‘å°±è¡Œ~*

